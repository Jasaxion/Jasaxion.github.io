<!DOCTYPE html>
<html lang="zh-CN">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>[é‡åŒ–|è®­ç»ƒ]Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs - Jasaxionä¸€åªå¤§é›„</title>
    <meta property="og:title" content="[é‡åŒ–|è®­ç»ƒ]Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs - Jasaxionä¸€åªå¤§é›„">
    
    <meta name="twitter:card" content="summary">
    <meta name="referrer" content="never" />

    
    
    <link rel="stylesheet" href="/css/custom.css"></link>
    <script type="text/javascript" src="/js/view-image.js"></script>
    <script>
    
    window.ViewImage && ViewImage.init('.main img:not(.avatar,.tk-avatar-img)')
    </script>
    <script src="/js/custom-markdown-parser.js"></script>
    <script  src="https://jsd.cdn.zzko.cn/npm/quicklink@2.3.0/dist/quicklink.umd.js"></script>
    <script>window.addEventListener('load', () => {  quicklink.listen();});</script>
    
    

    
      
    

    
      
      <meta property="description" content="https://arxiv.org/abs/2309.05516
æä¾›ä»£ç å®ç°ï¼šhttps://github.com/intel/auto-round
[&hellip;] å…¶ä»–æ¯”è¾ƒä¼˜ç§€çš„æ–¹æ³•ï¼šGPTQ: https://arxiv.org/abs/2210.17323 ICLR2023
[&hellip;] æ‘˜è¦éƒ¨åˆ†ã€Œä¹Ÿè¦å­¦ä¹ ä¸€ä¸‹å†™ä½œæ–¹æ³•ã€
Large Language Models &amp;hellip;">
      <meta property="og:description" content="https://arxiv.org/abs/2309.05516
æä¾›ä»£ç å®ç°ï¼šhttps://github.com/intel/auto-round
[&hellip;] å…¶ä»–æ¯”è¾ƒä¼˜ç§€çš„æ–¹æ³•ï¼šGPTQ: https://arxiv.org/abs/2210.17323 ICLR2023
[&hellip;] æ‘˜è¦éƒ¨åˆ†ã€Œä¹Ÿè¦å­¦ä¹ ä¸€ä¸‹å†™ä½œæ–¹æ³•ã€
Large Language Models &amp;hellip;">
      
    

    
    
    <meta name="twitter:image" content="https://pve.digikamc.cn:8343/i/2024/11/25/n8u5qm-0.png">
    
    

    

    
    

    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
    
  </head>

  
  <body class="posts">
    <header class="masthead">
      <h1><a href="/">Jasaxionä¸€åªå¤§é›„</a></h1>

<p class="tagline">é£æ‰“ï¼Œç¢ç‰ç’ƒï¼› æ‰“ä¸ç¢çš„ï¼Œæ˜¯é‚£é˜³å…‰æ¼«åœ°ã€‚The wind strikes, shattering the glazed glass; Yet unbroken remains the sunlight spilling across the earth.</p>

      <nav class="menu">
  <input id="menu-check" type="checkbox" hidden/>
  <label id="menu-label" for="menu-check" class="unselectable" hidden>
    <span class="icon close-icon">âœ•</span>
    <span class="icon open-icon">â˜°</span>
    <span class="text">Menu</span>
  </label>
  <ul>
  
  
  <li><a href="/posts/">å½’æ¡£ / Posts</a></li>
  
  <li><a href="/resume/">å…³äºæˆ‘ / CV</a></li>
  
  <li><a href="/tags/tech/">æŠ€æœ¯ç›®å½• / Tech</a></li>
  
  <li><a href="/tags/notes">æˆ‘çš„ç¬”è®° / Note</a></li>
  
  <li><a href="/tags/read/">ç§‘ç ”ä¸é˜…è¯» / Read</a></li>
  
  <li><a href="/whisper/">ç¢ç¢å¿µ / Whisper</a></li>
  
  
  </ul>
</nav>

      
    </header>

    <article class="main">
      <header class="title">
      
<h1>[é‡åŒ–|è®­ç»ƒ]Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</h1>



<h3>






2024-10-15
</h3>

<hr>


      </header>





<h2 id="intel-optimize-weight-rounding-via-signed-gradient-descent-for-the-quantization-of-llms">ã€Intelã€‘Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</h2>
<blockquote>
<p><a href="https://arxiv.org/abs/2309.05516">https://arxiv.org/abs/2309.05516</a></p>
<p>æä¾›ä»£ç å®ç°ï¼šhttps://github.com/intel/auto-round</p>
</blockquote>
<blockquote>
<p>å…¶ä»–æ¯”è¾ƒä¼˜ç§€çš„æ–¹æ³•ï¼šGPTQ: <a href="https://arxiv.org/abs/2210.17323">https://arxiv.org/abs/2210.17323</a> ICLR2023</p>
</blockquote>
<h3 id="æ‘˜è¦">æ‘˜è¦</h3>
<p>æ‘˜è¦éƒ¨åˆ†ã€Œä¹Ÿè¦å­¦ä¹ ä¸€ä¸‹å†™ä½œæ–¹æ³•ã€</p>
<p>Large Language Models (LLMs) have demonstrated exceptional proficiency in language related tasks, but their deployment poses significant challenges due to substantial memory and storage requirements. <strong>æå‡ºé—®é¢˜</strong>ï¼šå¤§æ¨¡å‹è™½ç„¶å¾ˆå¼ºï¼Œä½†éœ€è¦å¤§é‡çš„å­˜å‚¨éœ€æ±‚ã€‚</p>
<p><strong>Weight-only quantization</strong> has emerged as a promising solution, significantly reducing memory and storage needs without sacrificing too much performance. <strong>æŒ‡æ˜ä¸€èˆ¬è§£å†³æ–¹æ³•</strong>ï¼šæƒé‡é‡åŒ–</p>
<p>In this study, we introduce <strong>SignRound</strong>, a method that leverages signed gradient descent (SignSGD) to optimize rounding values and weight clipping in just 200 steps. SignRound integrates the advantages of <strong>Quantization-Aware Training (QAT)</strong> and <strong>Post-Training Quantization (PTQ),</strong> delivering exceptional results across 2 to 4 bits while minimizing tuning costs and avoiding additional inference overhead.  æŒ‡å‡ºæœ¬ç ”ç©¶çš„å†…å®¹ï¼šé‡‡ç”¨æœ‰ç¬¦å·çš„æ¢¯åº¦ä¸‹é™çš„æ–¹å¼ä¼˜åŒ–èˆå…¥å€¼å’Œæƒé‡è£å‰ªï¼Œé›†æˆäº†QAT å’Œ PTQ çš„ä¼˜åŠ¿ã€‚</p>
<p>For example, SignRound achieved absolute average accuracy improvements ranging from 6.91% to 33.22% at 2 bits, as measured by the average zero-shot accuracy across 11 tasks. It also demonstrates strong generalization in recent models, achieving near-lossless 4-bit quantization in most scenarios. ç‚¹å‡ºæœ¬ç ”ç©¶çš„æ•ˆæœï¼šåœ¨2bits é‡åŒ–çš„æ¡ä»¶ä¸‹ï¼Œæ€§èƒ½ä»6.91% åˆ°33.22%ï¼Œå¹¶ä¸”åœ¨4bit é‡åŒ–ä¸‹ï¼Œå‡ ä¹è¾¾åˆ°äº†æ— æŸæ€§èƒ½ã€‚</p>
<h3 id="ä»‹ç»">ä»‹ç»</h3>
<p><img src="https://pve.digikamc.cn:8343/i/2024/11/25/n8u5qm-0.png" alt="image-20241024120741569"></p>
<ol>
<li>
<p><strong>LLMéƒ¨ç½²æŒ‘æˆ˜</strong>ï¼š</p>
<ul>
<li>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡‡ç”¨æ˜¾è‘—å¢åŠ ï¼Œå³ä½¿åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šä¹Ÿæœ‰å¹¿æ³›éƒ¨ç½²éœ€æ±‚ã€‚</li>
<li>ç”±äºLLMå¯¹å†…å­˜å’Œå­˜å‚¨çš„é«˜è¦æ±‚ä»¥åŠè®¡ç®—éœ€æ±‚ï¼Œåœ¨è¿™äº›è®¾å¤‡ä¸Šéƒ¨ç½²LLMé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚</li>
</ul>
</li>
<li>
<p><strong>é‡åŒ–æŠ€æœ¯</strong>ï¼š</p>
<ul>
<li><strong>é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰</strong>ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿä½ç²¾åº¦è¡¨ç¤ºï¼Œä½¿æ¨¡å‹é€‚åº”é‡åŒ–æ•ˆæœï¼Œé€šå¸¸ç²¾åº¦è¾ƒé«˜ä½†å¤æ‚ä¸”èµ„æºæ¶ˆè€—å¤§ã€‚ã€Œè®­ç»ƒæ—¶é—´é•¿ã€</li>
<li><strong>è®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰</strong>ï¼šç›´æ¥é‡åŒ–æ¨¡å‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œç®€å•ä½†å¯èƒ½å¯¼è‡´ç²¾åº¦æ˜¾è‘—ä¸‹é™ã€‚ã€Œå¿«ï¼Œä½†ç²¾åº¦å¤ªä½ã€</li>
</ul>
</li>
<li>
<p><strong>é‡åŒ–ç±»å‹</strong>ï¼š</p>
<ul>
<li>é‡åŒ–å¯åº”ç”¨äºæ¿€æ´»å€¼å’Œæƒé‡ã€‚</li>
<li>å¯¹LLMçš„æ¿€æ´»å€¼è¿›è¡Œé‡åŒ–è¾ƒä¸ºå›°éš¾ï¼Œå› æ­¤<strong>æƒé‡é‡åŒ–</strong>æ›´ä¸ºå®ç”¨ã€‚</li>
<li>å†…å­˜å¸¦å®½é™åˆ¶ä½¿å¾—æƒé‡é‡åŒ–æ›´å…·ä¼˜åŠ¿ã€‚</li>
</ul>
</li>
<li>
<p><strong>æƒé‡é‡åŒ–</strong>ï¼š</p>
<ul>
<li>
<p>ç ”ç©¶é‡ç‚¹æ˜¯æƒé‡é‡åŒ–ã€‚å…³é”®æ­¥éª¤æ˜¯èˆå…¥ï¼Œé€šå¸¸é€šè¿‡æœ€è¿‘èˆå…¥ï¼ˆRTNï¼‰å®ç°ã€‚</p>
<p>ç¼ºç‚¹ï¼šRTNç‹¬ç«‹é‡åŒ–æ¯ä¸ªæƒé‡ï¼Œå¿½ç•¥æƒé‡é—´åŠæƒé‡ä¸æ¿€æ´»å€¼é—´çš„å…³ç³»ã€‚</p>
</li>
<li>
<p>è‡ªé€‚åº”èˆå…¥ï¼ˆAdaptive Roundingï¼‰æ¢ç´¢æ”¹è¿›èˆå…¥ç­–ç•¥ä»¥æé«˜ç²¾åº¦ï¼Œä½†å¯èƒ½å› æƒé‡æ˜¾è‘—å˜åŒ–è€Œæ•ˆæœä¸ä½³ã€‚</p>
</li>
</ul>
</li>
<li>
<p><strong>æœ¬ç ”ç©¶çš„ä¼˜åŒ–æ–¹æ³•</strong>ï¼š</p>
<ul>
<li>é€‰æ‹©SignSGDä½œä¸ºä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨æœ‰é™æ­¥éª¤å†…é€¼è¿‘æœ€ä¼˜èˆå…¥è§£ã€‚</li>
<li>SignSGDå› å…¶è§£ç©ºé—´è¾¹ç•Œæ˜ç¡®å’Œæ–¹æ³•ç®€å•è€Œé€‰å®šï¼Œä»…éœ€å°‘é‡è¶…å‚æ•°è°ƒæ•´ã€‚</li>
</ul>
</li>
<li>
<p><strong>è´¡çŒ®</strong>ï¼š</p>
<ul>
<li>æå‡ºä¸€ç§ç»“åˆQATå’ŒPTQä¼˜åŠ¿çš„æƒé‡é‡åŒ–ä¼˜åŒ–æ–¹æ³•ï¼Œå°†SignSGD åº”ç”¨åœ¨é‡åŒ–ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨2-bitåˆ°4-bitçš„ä¸åŒé‡åŒ–é…ç½®ä¸‹ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºè¿‘æœŸå·¥ä½œã€‚</li>
<li>è¯¥æ–¹æ³•å‘ç°å¦‚æœæ¡ä»¶çš„æ¨¡å‹çš„æ¨¡å‹è¶…å‚æ•°å¯ä»¥è¿›ä¸€æ­¥å¢å¼ºSignSGD çš„æ€§èƒ½ï¼Œåœ¨ä¸åŒæ¨¡å‹é—´å…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä½¿ç”¨4-bité‡åŒ–åœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹å‡ ä¹æ— æŸã€‚</li>
</ul>
</li>
</ol>
<h4 id="rounding-methods">Rounding Methods.</h4>
<ol>
<li>Adaptive Rounding: ä»–ä»¬é€šè¿‡æ³°å‹’çº§æ•°å±•å¼€æ¥è¿‘ä¼¼ä»»åŠ¡æŸå¤±ï¼Œå°†èˆå…¥ä»»åŠ¡ç”¨ä½œäºŒæ¬¡æ— çº¦æŸäºŒå…ƒä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>FlexRound: é€šè¿‡ç»“åˆå…ƒç´ åˆ’åˆ†ï¼Œå¼•å…¥äº†ä¸€ç§æ›´çµæ´»çš„å››èˆäº”å…¥æ–¹æ³•ã€‚</li>
<li>Oscillation-free: è¡¨æ˜å¼•å…¥å¯å­¦ä¹ å‚æ•°å¯èƒ½ä¼šå¯¼è‡´æƒé‡æŒ¯è¡é—®é¢˜ã€‚</li>
<li>AQuant: å¼•å…¥äº†ä¸€ç§åŠ¨æ€æ–¹æ³•ï¼Œå…¶ä¸­è¾¹ç•Œæˆä¸ºä¾èµ–äºæ¿€æ´»å€¼çš„å‡½æ•°ï¼Œä»¥å‡å°‘æ¿€æ´»çš„é‡åŒ–è¯¯å·®ã€‚</li>
<li>Signed Gradient Descent: å¸¦ç¬¦å·æ¢¯åº¦ä¸‹é™å¹¶ä¸å¸¸ç”¨ï¼Œé€šå¸¸åº”ç”¨äºç‰¹å®šåœºæ™¯ï¼Œä¾‹å¦‚é™ä½é€šä¿¡æˆæœ¬ã€‚è¿™æ˜¯å› ä¸ºä¸åŸå§‹æ¢¯åº¦ç›¸æ¯”ï¼Œå¸¦ç¬¦å·æ¢¯åº¦æºå¸¦çš„ä¿¡æ¯è¦å°‘å¾—å¤šã€‚æœ€è¿‘çš„ç ”ç©¶æ­ç¤ºäº†åœ¨æŸäº›æ¡ä»¶ä¸‹åŸºäºç¬¦å·çš„æ–¹æ³•ç›¸å¯¹äºæ¢¯åº¦ä¸‹é™çš„ä¼˜åŠ¿ã€‚å·´å‹’æ–¯ç­‰äººã€‚ ï¼ˆBalles et al., 2020ï¼‰å‘ç°ï¼Œå½“ Hessian çŸ©é˜µé›†ä¸­åœ¨å…¶å¯¹è§’çº¿ä¸Šä¸”æœ€å¤§ç‰¹å¾å€¼è¿œå¤§äºå¹³å‡ç‰¹å¾å€¼æ—¶ï¼ŒåŸºäºç¬¦å·çš„æ–¹æ³•æ›´å¯å–ã€‚æç­‰äººã€‚ (Li et al., 2023a) ç ”ç©¶äº†ä¸€ç§åŸºäºç¬¦å·çš„æ¢¯åº¦ä¸‹é™çš„å˜ä½“ï¼Œå®ƒè¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚è¨æ³•é‡Œå®‰ç­‰äººã€‚ ï¼ˆSafaryan å’Œ RichtÃ¡rikï¼Œ2021ï¼‰æå‡ºäº†ä¸€ç§å¸¦æœ‰åŠ¨é‡çš„éšæœºç¬¦å·ä¸‹é™ï¼Œå®ƒåœ¨æ ‡å‡†æœ‰ç•Œæ–¹å·®å‡è®¾ä¸‹ä»¥æœ€ä¼˜æ¸è¿‘ç‡æ”¶æ•›ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æœ‰ç¬¦å·æ¢¯åº¦ä¸‹é™æ–¹æ³•çš„æ½œåœ¨å¥½å¤„å’Œåº”ç”¨ã€‚</li>
</ol>
<h3 id="å®éªŒéƒ¨åˆ†">å®éªŒéƒ¨åˆ†</h3>
<p>è®ºæ–‡åŒ…å«äº†å¤§é‡ä¸”è¯¦ç»†çš„å®éªŒï¼Œè¿™æ˜¯åº”è¯¥å­¦ä¹ çš„ï¼Œç”¨ä¸°å¯Œçš„å®éªŒæ¥è®ºè¯å…¶æœ‰æ•ˆæ€§ï¼Œè€Œä¸æ˜¯å¤¸å¤§å…¶è¯ã€‚</p>
<p><img src="https://pve.digikamc.cn:8343/i/2024/11/25/n8u34c-0.png" alt="image-20241024121123559"></p>
<p>è¯¦ç»†çš„å®éªŒå°±ä¸è¿‡å¤šä»‹ç»äº†ï¼Œå¯ä»¥è¯¦ç»†å»çœ‹åŸè®ºæ–‡ã€‚</p>
<h3 id="æ€»ç»“">æ€»ç»“</h3>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SignRoundï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆè€Œç®€æ´çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡åŒ–ä¸­ä¼˜åŒ–æƒé‡å››èˆäº”å…¥ã€‚SignRoundä½¿ç”¨ç¬¦å·æ¢¯åº¦ä¸‹é™ï¼Œåœ¨200ä¸ªæ­¥éª¤ä¸­è°ƒæ•´å››èˆäº”å…¥å€¼å’Œé‡é‡å‰ªåˆ‡ï¼Œåœ¨å¤§çº¦2.5å°æ—¶å†…å®ŒæˆLLAMA-V2-70Bçš„é‡åŒ–ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°åœºæ™¯ä¸­ï¼ŒSignRoundåœ¨å„ç§æ¨¡å‹å’Œæƒé‡ä½æ–¹é¢çš„è¡¨ç°ä¼˜äºå…¶ä»–é‡åŒ–æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSignRoundåœ¨æœ€è¿‘çš„æ¨¡å‹ä¸­å±•ç¤ºäº†æœ‰å¸Œæœ›çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é€šè¿‡ç‰¹å®šäºæ¨¡å‹çš„è¶…å‚æ•°è°ƒæ•´å®ç°äº†å¢å¼ºçš„æ€§èƒ½ã€‚</p>
<h3 id="openreview-è¯„åˆ†ä»‹ç»-ç§‘ç ”æ€»æ˜¯å……æ»¡åå·">Openreview è¯„åˆ†ä»‹ç»ã€Œç§‘ç ”æ€»æ˜¯å……æ»¡åå·ã€‚ã€</h3>
<p><strong>è¦å‘reviewer ä¸€æ ·å»å®¡è§†è®ºæ–‡ï¼</strong></p>
<p>æ›´æ–°äº 2024.10.24ï¼Œ2024 ICLR reject</p>
<p><strong>æœ€ç»ˆè¯„ä»·</strong>ï¼šThis paper proposes a sign gradient descent method to optimize the weight rounding for quantizing llms, which achieves superior performance than gptq and awq for several common llms including opt, llama-1, and llama-2. One notable merit of this paper is its simplicity. <strong>Weaknesses include limited novelty, marginal improvements, and lack of comparisons over previous works AdaRound and FlexRound.</strong> Overall, the reasons to reject slightly overweighs the reasons to accept.</p>
<blockquote>
<p>ç¼ºç‚¹åŒ…æ‹¬<strong>æ–°é¢–æ€§æœ‰é™</strong>ã€<strong>æ”¹è¿›æœ‰é™</strong>ä»¥åŠç¼ºä¹ä¸ä¹‹å‰ä½œå“ Ada Round å’Œ Flex Round çš„æ¯”è¾ƒã€‚</p>
</blockquote>
<p>æ¯”è¾ƒæœ‰ä»·å€¼çš„è¯„è®ºï¼š</p>
<ul>
<li>
<p>ä¼˜ç‚¹</p>
<ul>
<li>
<p>The paper is easy to follow. æ€æƒ³å®¹æ˜“ç†è§£</p>
</li>
<li>
<p>The paper conducts a large number of experiments across various models, tasks and quantization setting. Further, it also consider state-of-the-art LLMs like Llama2 in addition to older ones like OPT and BLOOM.</p>
<ul>
<li>è®ºæ–‡åœ¨å„ç§æ¨¡å‹ã€ä»»åŠ¡å’Œé‡åŒ–è®¾ç½®ä¸Šè¿›è¡Œäº†å¤§é‡çš„å®éªŒã€‚æ­¤å¤–ï¼Œé™¤äº† OPT å’Œ BLOOM ç­‰è¾ƒæ—§çš„ä¹‹å¤–ï¼Œå®ƒè¿˜è€ƒè™‘äº† Llama2 ç­‰æœ€å…ˆè¿›çš„ LL Msã€‚</li>
</ul>
</li>
<li>
<p>SignRound appears to bring some performance improvements relative to GPTQ, in particular on smaller models and for zero-shot tasks.</p>
<ul>
<li>ç›¸å¯¹äº GPTQï¼ŒSign Round ä¼¼ä¹å¸¦æ¥äº†ä¸€äº›æ€§èƒ½æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå°çš„æ¨¡å‹å’Œé›¶æ ·æœ¬ä»»åŠ¡ä¸Šã€‚</li>
</ul>
</li>
<li>
<p>I also like that the paper includes also handful of <strong>unfavorable results</strong> to provide a more complete.</p>
<ul>
<li>è®ºæ–‡åŒ…å«äº†ä¸€äº›æ¨¡å‹ä¸å¤ªå¥½çš„ç»“æœã€Œä¸å¤¸å¤§å¹å˜˜ã€ï¼Œä½œä¸ºå®Œæ•´çš„ç»“æœå±•ç¤º</li>
</ul>
</li>
</ul>
</li>
<li>
<p>ç¼ºé™·</p>
<ul>
<li>The paper essentially seems to apply signed gradient descent (which is not new) to the standard layer-/block-wise rounding problem considered by various LLM PTQ papers. Hence, the overall novelty is low.
<ul>
<li>è¯¥è®ºæ–‡æœ¬è´¨ä¸Šä¼¼ä¹å°†<strong>æœ‰ç¬¦å·æ¢¯åº¦ä¸‹é™ï¼ˆè¿™å¹¶ä¸æ–°é²œï¼‰åº”ç”¨äºå„ç§ LLM PTQ è®ºæ–‡æ‰€è€ƒè™‘çš„æ ‡å‡†å±‚/å—èˆå…¥é—®é¢˜</strong>ã€‚å› æ­¤ï¼Œæ•´ä½“æ–°é¢–æ€§è¾ƒä½ã€‚</li>
</ul>
</li>
<li>GPTQ Activation-reordering can also be performed without any impact on inference performance (see the official GPTQ repo, option <code>--static-groups</code>). Further, if there is no grouping, reordering has no impact on inference. LLaMa1-7B and OPT-66B are known to be GPTQ outliers, for which reordering should be enabled to conduct a fair comparison.
<ul>
<li>è¿™ä¸ªè¯„è®ºå‘˜ç»å¯¹æ˜¯ä¸“å®¶ã€‚</li>
</ul>
</li>
<li>The paper argues that signed gradient descent is preferable over standard straight-through QAT (applied to the layer-wise quantization problem, like ZeroQuant) for this application, but does not provide any ablation studies supporting that point.
<ul>
<li>å¯å‘ï¼šè®ºæ–‡ä¸­æåˆ°çš„æ¯ä¸€ç‚¹ï¼Œéƒ½åº”è¯¥æœ‰ç†æœ‰æ®ï¼</li>
</ul>
</li>
<li>Based on Table 5, it appears that for the largest and most interesting models for compression applications, SignRound seems to perform very similar to GPTQ and in some cases even worse.</li>
<li>The code is not available in the Supplementary material.
<ul>
<li>ä»£ç å®ç°æ˜¯å¾ˆé‡è¦çš„</li>
</ul>
</li>
</ul>
</li>
</ul>



  <footer>
    <script src="https://immmmm.com/waterfall.min.js"></script>
    <script src="https://immmmm.com/imgStatus.min.js"></script>
    <script>
      document.addEventListener('DOMContentLoaded', () => {
      var photosAll = document.getElementsByTagName('gallery') || '';
      if(photosAll){
        for(var i=0;i < photosAll.length;i++){
          photosAll[i].innerHTML = '<div class="gallery-photos">'+photosAll[i].innerHTML+'</div>'
          var photosIMG = photosAll[i].getElementsByTagName('img')
          for(var j=0;j < photosIMG.length;j++){
            wrap(photosIMG[j], document.createElement('div'));
          }
        }
      }
      function wrap(el, wrapper) {
        wrapper.className = "gallery-photo";
        el.parentNode.insertBefore(wrapper, el);
        wrapper.appendChild(el);
      }
      let galleryPhotos = document.querySelectorAll('.gallery-photos') || ''
      if(galleryPhotos){
        imgStatus.watch('.gallery-photo img', function(imgs) {
          if(imgs.isDone()){
            for(var i=0;i < galleryPhotos.length;i++){
              waterfall(galleryPhotos[i]);
              let pagePhoto = galleryPhotos[i].querySelectorAll('.gallery-photo');
              for(var j=0;j < pagePhoto.length;j++){pagePhoto[j].className += " visible"};
            }
          }
        });
        window.addEventListener('resize', function () {
          for(var i=0;i < galleryPhotos.length;i++){
            waterfall(galleryPhotos[i]);
          }
        });
      }
    });
    </script>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/posts/2023/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%AE%97%E6%B3%95%E6%9C%BA%E8%AF%95%E6%8C%87%E5%8D%97/">è®¡ç®—æœºç®—æ³•æœºè¯•æŒ‡å—</a></span>
  <span class="nav-next"><a href="/posts/2024/10/%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BD%E7%A7%91%E7%A0%94%E8%AF%BB%E5%86%99%E5%9C%A8ph.d%E7%AC%AC0%E5%B9%B4%E7%9A%84%E6%89%80%E6%84%9F%E6%89%80%E5%8F%97/">å¦‚ä½•åšå¥½ç§‘ç ”ï¼Œè¯»ã€Šå†™åœ¨Ph.Dç¬¬0å¹´ã€‹çš„æ‰€æ„Ÿæ‰€å—</a> &rarr;</span>
</nav>

  <hr>
  
    <div class="pagination__title" style="margin-bottom: 30px;">
  <span class="pagination__title-h">ç•™ä¸‹ä½ çš„è¶³è¿¹ğŸ‘£</span>
</div>
  <div id="tcomment"></div>
  <script src="https://cdn.jsdelivr.net/npm/twikoo@1.6.5/dist/twikoo.all.min.js"></script>
  <script>
    twikoo.init({
    envId: 'https://mytwico.netlify.app/', 
    el: '#tcomment', 
    
    
    
  })
  </script>
  
  

  
  <hr>
  <div class="copyright">Â© <a href="https://jasaxion.github.io">Jasaxion</a> | since 2021</div>
  
  </footer>
  </article>
  
  </body>
</html>

