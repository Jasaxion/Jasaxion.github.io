<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>经验 on Jasaxion一只大雄</title>
    <link>http://localhost:1313/keywords/%E7%BB%8F%E9%AA%8C/</link>
    <description>Recent content in 经验 on Jasaxion一只大雄</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 01 Mar 2025 16:57:22 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/keywords/%E7%BB%8F%E9%AA%8C/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[2025|面经]商汤大模型见习研究员-offer</title>
      <link>http://localhost:1313/posts/2025/03/2025%E9%9D%A2%E7%BB%8F%E5%95%86%E6%B1%A4%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A7%81%E4%B9%A0%E7%A0%94%E7%A9%B6%E5%91%98-offer/</link>
      <pubDate>Sat, 01 Mar 2025 16:57:22 +0800</pubDate>
      <guid>http://localhost:1313/posts/2025/03/2025%E9%9D%A2%E7%BB%8F%E5%95%86%E6%B1%A4%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A7%81%E4%B9%A0%E7%A0%94%E7%A9%B6%E5%91%98-offer/</guid>
      <description>&lt;h2 id=&#34;起因&#34;&gt;起因&lt;/h2&gt;&#xA;&lt;p&gt;​&#x9;哇啦啦啦，研一下一开学，乌拉拉的一片人去实习了，本来我的意愿也不是很浓的，但最后左思右想后，还是决定也去尝试找一下实习，工程实习宁可不做，希望可以有一些成果产出的，科研类型的实习肯定优先。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[LLM 面试]手撕 BatchNorm 和 LayerNorm</title>
      <link>http://localhost:1313/posts/2025/02/llm-%E9%9D%A2%E8%AF%95%E6%89%8B%E6%92%95-batchnorm-%E5%92%8C-layernorm/</link>
      <pubDate>Wed, 26 Feb 2025 20:36:40 +0800</pubDate>
      <guid>http://localhost:1313/posts/2025/02/llm-%E9%9D%A2%E8%AF%95%E6%89%8B%E6%92%95-batchnorm-%E5%92%8C-layernorm/</guid>
      <description>&lt;h1 id=&#34;批量归一化-batch-normalization-bn&#34;&gt;批量归一化 (Batch Normalization, BN)&lt;/h1&gt;&#xA;&lt;p&gt;批量归一化是由Sergey Ioffe和Christian Szegedy在2015年提出的技术，目的是解决深度神经网络训练中的内部协变量偏移问题。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[实习面试]LLM 大模型岗位实习考点[RL 强化学习|多模态] （一）</title>
      <link>http://localhost:1313/posts/2025/02/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95llm-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B2%97%E4%BD%8D%E5%AE%9E%E4%B9%A0%E8%80%83%E7%82%B9rl-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%9A%E6%A8%A1%E6%80%81-%E4%B8%80/</link>
      <pubDate>Tue, 25 Feb 2025 12:13:43 +0800</pubDate>
      <guid>http://localhost:1313/posts/2025/02/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95llm-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B2%97%E4%BD%8D%E5%AE%9E%E4%B9%A0%E8%80%83%E7%82%B9rl-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%9A%E6%A8%A1%E6%80%81-%E4%B8%80/</guid>
      <description>&lt;h2 id=&#34;强化学习相关&#34;&gt;强化学习相关&lt;/h2&gt;&#xA;&lt;h3 id=&#34;ppo-算法&#34;&gt;PPO 算法&lt;/h3&gt;&#xA;&lt;p&gt;PPO 的思路：Actor + Critic（鼓励增长） + Clip （防止不良增长） + KL 惩罚（防止作弊行为，学习水平的不良增长）（Critic、Advantage、Clip、Reference Model）&lt;/p&gt;</description>
    </item>
    <item>
      <title>[LLM 面试] Numpy手写 Top P 和 Top K 算法</title>
      <link>http://localhost:1313/posts/2025/02/llm-%E9%9D%A2%E8%AF%95-numpy%E6%89%8B%E5%86%99-top-p-%E5%92%8C-top-k-%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 17 Feb 2025 15:22:13 +0800</pubDate>
      <guid>http://localhost:1313/posts/2025/02/llm-%E9%9D%A2%E8%AF%95-numpy%E6%89%8B%E5%86%99-top-p-%E5%92%8C-top-k-%E7%AE%97%E6%B3%95/</guid>
      <description>&lt;p&gt;大模型根据给定的输入文本（比如一个开头或一个问题）生成输出文本（比如一个答案或一个结尾）。&lt;strong&gt;为了生成输出文本，模型会逐个预测每个 token，直到达到一个终止条件（如一个标点符号或一个最大长度）。在每一步，模型会给出一个概率分布，表示它对下一个单词的预测概率。&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>[大模型面试]手推 Bert 模型的参数量</title>
      <link>http://localhost:1313/posts/2024/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E8%AF%95%E6%89%8B%E6%8E%A8-bert-%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F/</link>
      <pubDate>Wed, 27 Nov 2024 08:36:46 +0800</pubDate>
      <guid>http://localhost:1313/posts/2024/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E8%AF%95%E6%89%8B%E6%8E%A8-bert-%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;img src=&#34;https://pve.digikamc.cn:8343/i/2024/11/26/zgq3s3-0.jpeg&#34; alt=&#34;Bert 模型架构图- Genspark&#34; style=&#34;zoom:50%;&#34; /&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Bert 模型主要用的倒是左侧编码器部分&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;BERT-base 模型的主要架构参数：&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Layers (L) = 12 个Transformer层&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Hidden size (H) = 768&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Attention heads (A) = 12&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Vocabulary size (V) ≈ 30,522 (for BERT-base-uncased)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;这几个关键参数需要牢记&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
