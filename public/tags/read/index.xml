<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Read on Jasaxion一只大雄</title>
    <link>http://localhost:1313/tags/read/</link>
    <description>Recent content in Read on Jasaxion一只大雄</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 25 Oct 2024 13:59:02 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/read/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>如何做好科研，读《写在Ph.D第0年》的所感所受</title>
      <link>http://localhost:1313/posts/2024/10/%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BD%E7%A7%91%E7%A0%94%E8%AF%BB%E5%86%99%E5%9C%A8ph.d%E7%AC%AC0%E5%B9%B4%E7%9A%84%E6%89%80%E6%84%9F%E6%89%80%E5%8F%97/</link>
      <pubDate>Fri, 25 Oct 2024 13:59:02 +0800</pubDate>
      <guid>http://localhost:1313/posts/2024/10/%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BD%E7%A7%91%E7%A0%94%E8%AF%BB%E5%86%99%E5%9C%A8ph.d%E7%AC%AC0%E5%B9%B4%E7%9A%84%E6%89%80%E6%84%9F%E6%89%80%E5%8F%97/</guid>
      <description>&lt;p&gt;推荐阅读：&lt;a href=&#34;https://zhuanlan.zhihu.com/p/960781637&#34;&gt;《写在Ph.D第0年：AI/CV科研菜鸟的持续进阶之路》&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;选择自己的职业人生-工程师素质拓展课程&#34;&gt;选择自己的职业人生「工程师素质拓展课程」&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;可选择： 不要画地为牢&lt;/li&gt;&#xA;&lt;li&gt;什么是好的投资思维——“一眼就知道好不好”&lt;/li&gt;&#xA;&lt;li&gt;看到别人看不到的收益&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;开端&#34;&gt;开端&lt;/h3&gt;&#xA;&lt;p&gt;科研的本质实际上是项目管理「一人要身兼 算法、开发、产品、运营（宣传）、项目排期与进展管理」&lt;/p&gt;</description>
    </item>
    <item>
      <title>[量化|训练]Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</title>
      <link>http://localhost:1313/posts/2024/10/%E9%87%8F%E5%8C%96%E8%AE%AD%E7%BB%83optimize-weight-rounding-via-signed-gradient-descent-for-the-quantization-of-llms/</link>
      <pubDate>Tue, 15 Oct 2024 14:04:59 +0800</pubDate>
      <guid>http://localhost:1313/posts/2024/10/%E9%87%8F%E5%8C%96%E8%AE%AD%E7%BB%83optimize-weight-rounding-via-signed-gradient-descent-for-the-quantization-of-llms/</guid>
      <description>&lt;h2 id=&#34;intel-optimize-weight-rounding-via-signed-gradient-descent-for-the-quantization-of-llms&#34;&gt;【Intel】Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.05516&#34;&gt;https://arxiv.org/abs/2309.05516&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;提供代码实现：https://github.com/intel/auto-round&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;其他比较优秀的方法：GPTQ: &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;https://arxiv.org/abs/2210.17323&lt;/a&gt; ICLR2023&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
