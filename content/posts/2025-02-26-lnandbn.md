+++
date = '2025-02-26T20:36:40+08:00'
draft = false
title = '[LLM 面试]手撕 BatchNorm 和 LayerNorm'
tags = 'notes'
show_toc = false
twikoo = true
keywords = ['面试', '经验', '']
description = "[LLM 面试]手撕 BatchNorm 和 LayerNorm"

+++

# 批量归一化 (Batch Normalization, BN)

批量归一化是由Sergey Ioffe和Christian Szegedy在2015年提出的技术，目的是解决深度神经网络训练中的内部协变量偏移问题。

BN的计算过程如下：

```python
def batch_norm(x, gamma, beta, eps=1e-5):
    # x: 输入数据，形状为[N, D]，N为批量大小，D为特征维度
    # gamma, beta: 可学习的缩放和偏移参数
    
    # 步骤1: 计算批量均值
    batch_mean = np.mean(x, axis=0)
    
    # 步骤2: 计算批量方差
    batch_var = np.var(x, axis=0)
    
    # 步骤3: 标准化
    x_norm = (x - batch_mean) / np.sqrt(batch_var + eps)
    
    # 步骤4: 缩放和偏移
    out = gamma * x_norm + beta
    
    return out
```

BN的特点：
- 沿着批量维度计算统计量（均值和方差）
- 对于CNN，通常应用于[N, C, H, W]的数据，统计维度为[N, H, W]
- 训练时使用小批量统计量，测试时使用整个训练集的运行统计量
- 优点：加速训练、允许更高学习率、减少对初始化的敏感性
- 缺点：对小批量size敏感，RNN中效果不佳

# 层归一化 (Layer Normalization, LN)

层归一化是由Jimmy Lei Ba等人在2016年提出的，旨在解决BN在RNN和小批量情况下的问题。

LN的计算过程如下：

```python
def layer_norm(x, gamma, beta, eps=1e-5):
    # x: 输入数据，形状为[N, D]
    # gamma, beta: 可学习的缩放和偏移参数，形状为[D]
    
    # 步骤1: 计算每个样本的均值
    mean = np.mean(x, axis=1, keepdims=True)
    
    # 步骤2: 计算每个样本的方差
    var = np.var(x, axis=1, keepdims=True)
    
    # 步骤3: 标准化
    x_norm = (x - mean) / np.sqrt(var + eps)
    
    # 步骤4: 缩放和偏移
    out = gamma * x_norm + beta
    
    return out
```

LN的特点：
- 沿着特征维度计算统计量，而不是批量维度
- 对于每个样本独立计算归一化参数
- 训练和测试阶段行为一致，不需要维护运行统计量
- 优点：适用于RNN和小批量场景，不依赖批量大小
- 缺点：在CNN中效果可能不如BN

# BN与LN的关键区别

1. **归一化维度**：
   - BN：对批量中的每个特征独立归一化（跨样本）
   - LN：对每个样本的所有特征进行归一化（跨特征）

2. **应用场景**：
   - BN：适合CNN等固定大小输入的网络 CV 因为不同通道之间可能存在关联，CV来说BN更好
   - LN：适合RNN、Transformer等变长序列模型 NLP天然优势

3. **计算依赖**：
   - BN：依赖批量数据的统计特性，批量大小影响结果
   - LN：只依赖单个样本的统计特性，与批量大小无关

4. **训练/测试行为**：
   - BN：训练和测试阶段行为不同（训练用批量统计量，测试用全局统计量）
   - LN：训练和测试阶段行为一致

这两种归一化技术都是深度学习中的基础组件，在不同场景下各有优势。如今还有其他变体如Instance Normalization、Group Normalization等，它们在特定任务中可能表现更佳。

<img src="https://pve.digikamc.cn:8343/i/2025/02/26/z7liw7-0.png" alt="image-20250226212905061" style="zoom:50%;" />
