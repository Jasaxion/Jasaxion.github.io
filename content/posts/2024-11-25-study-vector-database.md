+++
date = '2024-11-25T14:13:33+08:00'
draft = true
title = '向量数据库学习（Vector Database 101）'
tags = 'notes'
show_toc = true
twikoo = true
math = true
keywords = ['Vector Database', '向量数据库', '检索增强']
description = "本篇博客详细学习了向量数据库的课程 Vector Database 101"

+++

## 向量数据库学习（Vector Database 101）

> 课程链接🔗：https://zilliz.com/learn/vector-index

### 非结构化数据介绍

- 非结构化数据指的是不符合预定义格式或无法适应结构化数据模型的信息。非结构化数据的例子包括人类生成的内容，如图片、视频、音频录音和文本文件。

- 结构化数据，另一方面，指的是可以存储在基于表格的格式中的数据，而半结构化数据指的是可以存储在单级或多级数组/键值存储中的数据

> Example:
>
> 结构化数据：代表数据库有MySQL、PostgreSQL
>
> | ISBN       | Year | Name                                 | Author        |
> | :--------- | :--- | :----------------------------------- | :------------ |
> | 0767908171 | 2003 | A Short History of Nearly Everything | Bill Bryson   |
> | 039516611X | 1962 | Silent Spring                        | Rachel Carson |
> | 0374332657 | 1998 | Holes                                | Louis Sachar  |
>
> 半结构化数据：代表数据集有NoSQL(宽列存储、对象/文档数据库、键值存储等),Cassandra，MongoDB，Redis
>
> ```json
> {
> ISBN: 0767908171
> Month: February
> Year: 2003
> Name: A Short History of Nearly Everything
> Author: Bill Bryson
> Tags: geology, biology, physics
> },
> {
> ISBN: 039516611X
> Name: Silent Spring
> Author: Rachel Carson
> },
> {
> ISBN: 0374332657
> Year: 1998
> Name: Holes
> Author: Louis Sachar
> },
> ...
> ```

#### 机器生成的非结构化数据

1. **传感器数据**：
   - 从传感器收集的数据，如温度传感器、湿度传感器、GPS传感器和运动传感器的数据。

2. **机器日志数据**：
   - 由机器、设备或应用程序生成的数据，包括系统日志、应用程序日志和事件日志。

3. **物联网（IoT）数据**：
   - 从智能设备收集的数据，如智能恒温器、智能家居助手和可穿戴设备。

4. **计算机视觉数据**：
   - 由计算机视觉技术生成的数据，如图像识别、物体检测和视频分析。

5. **自然语言处理（NLP）数据**：
   - 由NLP技术生成的数据，如语音识别、语言翻译和情感分析。

6. **网络和应用程序数据**：
   - 由网络服务器、网络应用程序和移动应用程序生成的数据，包括用户行为数据、错误日志和应用程序性能数据。

#### 人类生成的非结构化数据

1. **电子邮件**：
   - 电子邮件通常是非结构化的，可能包含自由格式的文本、图像和附件。

2. **短信**：
   - 短信可以是非正式的、非结构化的，并包含缩写或表情符号。

3. **社交媒体帖子**：
   - 社交媒体帖子在结构和内容上可能有所不同，包括文本、图像、视频和标签。

4. **音频记录**：
   - 人类生成的音频记录，如电话通话、语音邮件、音频文件和音频笔记，都是非结构化数据。

5. **手写笔记**：
   - 手写笔记可能包含非结构化的文本、绘图、图表和其他视觉元素。

6. **会议笔记**：
   - 会议笔记可能包含非结构化的文本、图表和行动项。

7. **转录文本**：
   - 演讲、采访和会议的转录文本可能包含非结构化的文本，准确度各不相同。

8. **用户生成内容**：
   - 网站和论坛上的用户生成内容可能包含非结构化的数据，如自由格式的文本、图像和视频文件。

ANN搜索是向量数据库的核心组件，同时也是一项庞大的研究领域，因此这本质上是一个具备概率性的查询过程，但是结构化/半结构化的数据查询是确定的。

### 向量数据库的介绍

向量数据库是一种新型数据库系统，专门用于存储、索引和快速搜索高维向量嵌入，以实现高效的语义信息检索和向量语义搜索。

#### 关键功能

- **存储与索引**：管理高维向量数据。
- **快速搜索**：支持快速语义信息检索和向量语义搜索。

#### 应用场景

向量数据库广泛应用于以下领域：

- **聊天机器人**
- **推荐系统**
- **图像/视频/音频搜索**
- **语义搜索**
- **检索增强生成（RAG）**

#### 技术背景

- **RAG架构**：作为现代AI技术栈的关键组件，RAG通过提供外部知识来增强大型语言模型（LLM）的输出，减少AI幻觉现象。
- **外部知识存储**：向量数据库存储这些外部知识，并为LLM提供上下文信息，以生成更准确的答案。

#### 主流向量数据库

- **专用向量数据库**：
  - Milvus
  - Zilliz Cloud（全托管的Milvus）
  - Qdrant
  - Weaviate
  - Pinecone
  - Chroma
- **传统数据库的向量插件**：
  - Cassandra
  - MongoDB
  - Pgvector

这些数据库通过添加向量插件，能够执行小规模的向量搜索。

#### 向量数据库是如何工作的

向量数据库如Milvus和Zilliz（完全托管Milvus）是专门设计用于存储、处理、索引和搜索向量嵌入。大多数向量数据库支持主流索引，如层次可导航小世界（HNSW）、局部敏感哈希（LSH）和产品量化（PQ）。换句话说，向量数据库主要在向量嵌入上操作，并与将此数据转换为向量嵌入的机器学习模型紧密协作。

![img](https://pve.digikamc.cn:8343/i/2024/11/25/ndjyh1-0.png)

> 1. 机器学习模型（通常是嵌入模型）将各种非结构化数据转换为向量嵌入。
> 2. 这些向量嵌入存储在Zilliz Cloud中。
> 3. 用户执行查询。
> 4. 机器学习模型将查询转换为向量嵌入。
> 5. Zilliz Cloud通过使用近似最近邻（ANN）算法比较查询向量与数据集中向量之间的距离，进行向量搜索。
> 6. Zilliz Cloud找到与查询最相关的Top-K结果。
> 7. Zilliz Cloud将结果返回给用户。

对于一个完整的向量数据库来说，向量数据库应该具备以下特性：1）可扩展性和可调性，2）多租户和数据隔离，3）完整的访问控制API套件，以及4）直观的用户界面/管理控制台。

#### 当前向量数据库所遇到的挑战

1. **如何设计出一个灵活度高和可扩展的数据模型很关键**：Snowflake这样的公司已经向更广泛的存储行业表明，“无共享”数据库架构在某种程度上优于传统的“共享存储”云数据库模型。
2. 有了数据模型，那么向量查询和index 索引就成为下一个关键的部分。向量索引和查询同样计算密集，在加速器上运行时达到最大速度和效率。这一系列计算资源为第二个主要技术挑战让路，即开发异构计算架构。
3. 最终的主要挑战是开发一套API和GUI，在利用现有用户界面惯例的同时，保持与底层架构的兼容性。

#### 向量数据库的优点

向量数据库相较于传统关系型数据库和主流向量数据库，在涉及向量相似性搜索、语义搜索、机器学习和人工智能应用的场景中具有显著优势。以下是向量数据库的一些主要优点：

1. **高维搜索**：向量数据库能够高效地进行高维向量的相似性搜索，这在机器学习和生成式人工智能（GenAI）应用中非常常见，如图像识别、自然语言处理和推荐系统。它们能够快速找到与给定查询最相似的数据点，这对于推荐引擎、图像识别和自然语言处理等应用至关重要。

2. **可扩展性**：向量数据库能够水平扩展，高效地存储和检索大量高维向量。这对于需要实时搜索和检索大量数据的应用来说非常重要。

3. **混合搜索的灵活性**：向量数据库能够处理各种向量数据类型，包括稀疏和密集向量。它们还可以处理多种数据类型，包括数值、文本和二进制数据。

4. **性能**：向量数据库能够高效地执行向量相似性搜索，通常比传统数据库提供更快的搜索时间。

5. **可定制的索引**：向量数据库允许为特定用例和数据类型定制索引方案。

总的来说，向量数据库在涉及高效相似性搜索、语义搜索、复杂数据和机器学习的应用中提供了显著优势，能够在向量空间中快速高效地搜索和检索高维向量数据。

> 评测基准：ANN Benchmark: http://ann-benchmarks.com/
>
> 数据集与参数设定：基准测试提供了多种规模和维度的数据集，并为每个数据集配备了一组参数，如搜索的近邻数量和使用的距离度量方法。
>
> 搜索召回率计算：基准测试计算搜索召回率，即在返回的k个近邻中找到真实最近邻的查询比例。搜索召回率是评估最近邻搜索算法准确性的关键指标。
>
> RPS计算：基准测试还计算RPS（每秒查询数），即向量数据库或搜索算法处理查询的速度。这一指标对于评估系统的速度和可扩展性至关重要。

### 向量数据库相似度搜索

#### 向量嵌入的比较策略

1. 文本首先转化为嵌入表示
2. 计算相似度「余弦相似度、点积相似度」

#### 向量搜索策略

现在我们已经看到了向量嵌入的强大功能，让我们简要地看看我们可以进行最近邻搜索的一些方法。这不是一个详尽的列表；我们只是简要地概述一些常见方法，以便提供一个关于如何进行大规模向量搜索的高级概述。请注意，这些方法并非互斥——例如，可以将量化与空间划分结合使用。

##### 线性搜索

> 策略方式是逐个元素进行比较

最简单但最天真的最近邻搜索算法是古老的线性搜索：计算查询向量到向量数据库中所有其他向量的距离。

显然，当试图将我们的向量数据库扩展到数千万乃至数亿条向量时，朴素搜索方法并不适用。然而，在数据库中元素总数较少的情况下，这实际上可能是最有效的向量搜索方式，因为无需构建独立的索引数据结构，同时插入和删除操作也能相对容易地实现。由于朴素搜索不存在空间复杂度问题，且没有持续的空间开销，这种方法在查询中等数量向量时，往往能胜过空间划分技术。

##### 空间划分

空间划分不是一个单一的算法，而是一系列使用相同概念的算法集合。

1.  **KD树（K 维树）**：它通过类似于二叉搜索树的方式，不断地将搜索空间二分（将向量分为“左”和“右”桶）。
2.  **倒排文件索引（IVF）**也是一种空间划分形式，它通过将每个向量分配到其最近的质心来工作——搜索是通过首先确定查询向量的最近质心并在那里进行搜索来进行的，这显著减少了需要搜索的总向量数。IVF是一种相当流行的索引策略，通常与其他索引算法结合使用以提高性能。

##### 量化

量化是一种通过降低向量的精度来减少数据库总大小的技术

1. 标量量化（SQ）通过将高精度浮点向量与一个标量值进行相乘，然后将结果向量的元素转化为它们最近的整数来实现，不仅减少了整个数据库的有效大小，还具有加快向量到向量距离计算的积极作用
2. 乘积量化 PQ，是另外一种量化方式，在 PQ 量化中所有的向量被划分为等大小的块，并且每个子块都被获得一个质心

##### Hierarchical Navigable Small Words(HNSW)

层次可导航小世界是一种基于图的索引和检索算法，这与乘积量化的工作方式不同：不是通过减少数据库的有效大小来提高其可搜索性，HNSW 通过原始数据创建一个多层图。上层只包含“长连接”，而下层只包含数据库中向量的“短连接 ”，单个图的连接是按照跳表的方式创建的，有了这种架构，搜索变得相当直接——我们首先贪心地遍历最顶层的图以找到我们地查询向量最接近地向量，然后我们对第二层图也进行同样地操作，使用第一层图的结果作为起点，这个过程一直 持续到我们在最底层的图完成搜索，其结果成为查询向量的最近邻。

<img src="https://pve.digikamc.cn:8343/i/2024/11/25/ne0tsk-0.png" alt="HNSW, visualized. Image source: https://arxiv.org/abs/1603.09320" style="zoom:60%;" />

##### 最近邻搜索 ANN

ANNOY的工作原理是首先在数据库中随机选择两个向量，并沿着这两个向量之间的超平面将搜索空间一分为二。这个过程会迭代进行，直到每个节点中的元素数量少于某个预定义的参数NUM_MAX_ELEMS。由于生成的索引本质上是一个二叉树，这使得我们能够在O(log n)的复杂度下进行搜索。

#### 常用的相似度指标

##### 浮点向量相似度度量 

最常见的浮点向量相似度度量方法包括L1距离、L2距离和余弦相似度，这些方法并无特定顺序。前两者为距离度量（数值越小表示越相似，数值越大则表示越不相似），而余弦相似度则是一种相似度度量（数值越大表示越相似）。

L1距离通常也被称为曼哈顿距离，这一名称恰如其分地反映了在曼哈顿从A点到B点需要沿着两个垂直方向之一移动的事实。第二个公式，L2距离，是欧几里得空间中两个向量之间的简单距离。第三个也是最后一个公式是余弦距离，等同于两个向量之间夹角的余弦值。注意，余弦相似度的公式实际上是输入向量a和b的归一化版本的点积。通过一些数学推导，我们还可以证明，对于单位范数的向量，L2距离和余弦相似度在相似性排序上是实质等效的：

##### 二进制向量相似度度量

二进制向量，顾名思义，不具备基于浮点向量算术运算的度量标准。二进制向量的相似度度量依赖于集合数学、位操作或两者的结合（没关系，我也讨厌离散数学）。以下是两种常用的二进制向量相似度度量的公式：

<img src="https://pve.digikamc.cn:8343/i/2024/11/25/ne3zqc-0.png" alt="image-20241104123805869" style="zoom:50%;" />

第一个方程称为Tanimoto/Jaccard距离，本质上衡量的是两个二进制向量之间的重叠程度。第二个方程是汉明距离，计算的是向量a和b中元素不同的数量。在大多数应用中，人们更倾向于使用浮点嵌入上的余弦相似度，因此你很可能可以安全地忽略这些相似度度量。

### Index 向量索引

Milvus 使用FAIS 作为关键索引构建库+HNSWlib+ANNOY

#### 向量索引的类型

- 基于哈希的索引方式：locality-sensitive hashing
- 基于树的索引方式：ANNOY
- 基于聚类的索引方式：乘积量化PQ
- 基于图的索引方式：HNSW，CAGRA

不同类型的算法在处理大数据集和各种类型的向量数据时表现更佳，但它们都以牺牲一定的准确性/召回率为代价，来加速向量搜索过程。

一个经常被忽视的矢量搜索关键细节是能够将许多这样的矢量索引和搜索算法组合在一起。在一个矢量数据库中，一个完整的矢量索引通常由三个不同的组件组成：

1. 【嵌入表示】一个**可选的预处理步骤，其中矢量在索引之前可能被减少或优化；**

   在这一步中主要是处理向量，例如使用PLM 完成文本嵌入表示，包括L2 归一化、降维、和零填充等，大部分的向量数据库跳过了这一步，将预处理的步骤交给了用户层面进行处理。

2. 【构建索引】一个必需的主要步骤，即用于**索引的核心算法；**

   这是必须组件，为向量索引的核心。这一步应该输出一个数据结构，它包含可以进行高效向量搜索所需要的所有信息。通常使用基于树/图的数据结构。但量化算法或基于哈希的索引（如乘积量化或局部敏感哈希）同样适用。

3. 【量化】一个可选的次要步骤，**其中矢量可能被量化或散列以进一步提高搜索速度；**

   通过将数据集中所有浮点数值映射为低精度整数值**，即从float64到int8或float32到int8，来缩减索引的总规模**。这一改动既能减小索引尺寸，又能提升搜索速度，但通常会以牺牲一定精度为代价。

#### Flat Index

Flat Index大体上是最基础的索引策略，但可能也是最容易被忽视的。在扁平索引中，我们将查询向量与数据库中的每一个其他向量进行比较。

#### IVF Inverted File Index

倒排文件索引通过将整个数据集中的向量空间划分为多个分区，从而缩小了整体搜索范围。每个分区都关联一个质心，数据集中的每个向量则被分配到与其最近质心对应的分区中。

> <img src="https://pve.digikamc.cn:8343/i/2024/11/25/ne5tfc-0.png" alt="A two-dimensional Voronoi diagram. Image by Balu Ertl, CC BY-SA 4.0." style="zoom:20%;" />

### 标量量化Scalar quantization

标量量化究竟是如何工作的呢？首先，我们来看一下索引过程，即将浮点向量转换为整数向量的步骤。对于每个向量维度，标量量化会取该维度在整个数据库中的最大值和最小值，并在其整个范围内均匀地将其划分为若干个区间。

实现过程「这是一类线性转化过程，也可以使用指数转化或二次转化」

```python
import numpy as np

class ScalarQuantizer:

    def __init__(self):
        self._dataset = None

    def create(self):
        """Calculates and stores SQ parameters based on the input dataset."""
        self._dtype = dataset.dtype  # original dtype
        self._starts = np.min(dataset, axis=1)
        self._steps = (np.max(dataset, axis=1) - self._starts) / 255

        # the internal dataset uses `uint8_t` quantization
        self._dataset = np.uint8((dataset - self._starts) / self._steps)

    def quantize(self, vector):
        """Quantizes the input vector based on SQ parameters"""
        return np.uint8((vector - self._starts) / self._steps)

    def restore(self, vector):
        """Restores the original vector using SQ parameters."""
        return (vector * self._steps) + self._starts

    @property
    def dataset(self):
        if self._dataset:
            return self._dataset
        raise ValueError("Call ScalarQuantizer.create() first")
```

### 乘积量化Product Quantization

标量量化的一个主要缺点是它没有考虑每个维度中值的分布。

> ``` 
> array([[ 9.19,  1.55],
>     [ 0.12,  1.55],
>     [ 0.40,  0.78],
>     [-0.04,  0.31],
>     [ 0.81, -2.07],
>     [ 0.29,  0.82],
>     [ 0.05,  0.96],
>     [ 0.12, -1.10]])
> ```
>
> 如果我们决定将这些向量量化为3位整数（范围[0,7]），0维度的6个桶将完全未被使用！显然，必须有一种更好的量化方法，尤其是如果任何维度具有非均匀分布的话。

PQ背后的主要思想是将高维向量算法性地分割成低维子空间，子空间的维度与原始高维向量中的多个维度相对应。这个过程通常使用一种特殊的算法，称为Lloyd算法，这是一种量化器，其效果等同于k-means聚类。与标量量化类似，每个原始向量在量化后都产生一个整数向量，每个整数对应一个特定的质心。

![product_quantization.png](https://pve.digikamc.cn:8343/i/2024/11/25/ndjy9r-0.png)

1. 给定一个包含N个向量的数据集，我们首先将每个向量分割成M个子向量（也称为子空间）。这些子向量的长度不一定相同，但在实际应用中它们几乎总是相同的。
2. 接着，我们对数据集中的所有子向量使用k-means算法（或其他聚类算法）。这将为我们提供每个子空间的一组K个质心，每个质心都会被分配一个唯一的ID。
3. 在计算出所有质心后，我们将原始数据集中的所有子向量替换为其最接近质心的ID。

代码逻辑实现：

```python
import numpy as np
from scipy.cluster.vq import kmeans2


class ProductQuantizer:

    def __init__(self, M=16, K=256):
        self.M = 16
        self.K = 256
        self._dataset = None

    def create(self, dataset):
        """Fits PQ model based on the input dataset."""
        sublen = dataset.shape[1] // self.M
        self._centroids = np.empty((self.M, self.K, sublen), dtype=np.float64)
        self._dataset = np.empty((dataset.shape[0], self.M), dtype=np.uint8)
        for m in range(self.M):
            subspace = dataset[:,m*sublen:(m+1)*sublen]
            (centroids, assignments) = kmeans2(subspace, self.K, iter=32)
            self._centroids[m,:,:] = centroids
            self._dataset[:,m] = np.uint8(assignments)

    def quantize(self, vector):
        """Quantizes the input vector based on PQ parameters"""
        quantized = np.empty((self.M,), dtype=np.uint8)
        for m in range(self.M):
            centroids = self._centroids[m,:,:]
            distances = np.linalg.norm(vector - centroids, axis=1)
            quantized[m] = np.argmin(distances)
        return quantized

    def restore(self, vector):
        """Restores the original vector using PQ parameters."""
        return np.hstack([self._centroids[m,vector[m],:] for m in range(M)])

    @property
    def dataset(self):
        if self._dataset:
            return self._dataset
        raise ValueError("Call ProductQuantizer.create() first")
```

### HNSW

核心内容：

- Probability skip list 关键跳表
- NSW 可导航的小世界

#### 什么是跳过的概率表（ the probability skip lists）

相比于单链表，这种数据结构引入了额外的层次来解决单链表的随机访问O(n)时间复杂度上面的问题，实现O(logn)的时间复杂度来完成随机元素的访问。代价是空间复杂度为O(nlogn)，多了logn*。并且在插入和删除操作时会带来更多一些的运行时开销。

概率跳表是一种多层次的链表结构，上层维护着较长的连接，随着逐层向下移动，其连接会越来越短，而最底层是一个包含了所有元素的“原始”链表：

![skip list structure](https://pve.digikamc.cn:8343/i/2024/11/25/ndjybf-0.png)



> 我们从跳表的最高层开始，以找到元素i。一旦我们发现一个节点对应列表中的元素大于i，我们就回溯到前一个节点并移至下一层。这一过程持续进行，直到我们找到所需的元素。需要注意的是，跳表仅适用于已排序的列表，因为我们需直接比较两个对象的大小。
>
> 插入操作具有概率性。对于任何新元素，我们首先需要确定该元素首次出现的层级。最顶层的出现概率最低，随着层级向下移动，概率逐渐增加。一般规则是，某一层中的任何元素将以预定义的概率p出现在其上一层。因此，如果一个元素首次出现在某一层l，它也将被添加到l-1层、l-2层，依此类推。

#### NSW

**NSW（Navigable Small Worlds）是一种基于图的算法，**用于在数据集中寻找近似最近邻。其基本思想是：**首先设想一个网络中有许多节点，每个节点都有短程、中程和长程连接到其他节点。**在进行向量搜索时，我们从某个预定义的入口点开始，评估与其它节点的连接，并跳转到最接近目标节点的那个节点。这个过程重复进行，直到找到最近的邻居。这种搜索方式被称为贪婪搜索。

NSW算法在小规模（数百或数千个节点）的NSW中表现良好，但在更大规模的NSW中，性能会下降。为了解决这个问题，可以**通过增加每个节点的短程、中程和长程连接的平均数量来改进，但这会增加网络的整体复杂性，并导致搜索时间变长**。在最坏的情况下，**即每个节点都连接到数据集中的所有其他节点时，NSW的性能并不比朴素**（线性）搜索更好。

#### HNSW

HNSW通过借鉴跳表的概念扩展了NSW。与跳表类似，HNSW维护了多个层次（因此称为分层可导航小世界），只不过这些层次是由NSW而非链表构成。HNSW图的最顶层节点数量较少，连接距离最长，而最底层则包含所有节点，连接距离最短。在搜索过程中，我们从最顶层的一个预定义点开始，贪婪地向查询向量的最近邻节点进行路由。一旦到达最近的节点，我们便在HNSW图的第二层重复这一过程。如此持续，直至找到最近的邻居节点。

> HNSW（Hierarchical Navigable Small World）算法是对NSW（Navigable Small World）算法的扩展，借鉴了跳表（Skip List）的概念。HNSW通过维护多个层次的NSW图来实现高效的近邻搜索。以下是HNSW算法的主要特点和步骤：
>
> 1. 层次结构
>
> - **多层图结构**：HNSW由多个层次的NSW图组成，最上层节点少且连接距离长，最下层包含所有节点且连接距离短。
> - **层间关系**：每一层都是一个NSW图，上层图是下层图的子集，上层图中的节点是下层图中节点的子集。
>
> 2. 搜索过程
>
> - **自顶向下搜索**：搜索从最上层开始，贪婪地向查询向量的最近邻居前进。到达当前层的最近节点后，进入下一层继续搜索，直到到达最底层。
> - **贪婪路由**：在每一层中，算法通过贪婪路由策略（即总是选择当前最近的邻居）来快速接近目标。
>
> 3. 插入过程
>
> - **逐层插入**：对于新插入的向量v，首先在最上层找到其最近邻居，然后逐层向下，在每一层中找到v的最近邻居，直到最底层。
> - **链接创建**：在每一层中，根据预定义的参数M（最大双向链接数），为v创建链接。通常这些链接指向v的最近邻居，但也可以使用其他启发式方法。
>
> 4. 层间概率分布
>
> - **层间分布**：向量出现在上层的概率呈指数递减。具体来说，HNSW使用公式 `floor(-ln(rand(0, 1)))` 来决定向量出现在哪一层，其中 `rand(0, 1)` 是从均匀分布中抽取的随机数。
> - **图结构质量**：尽管这种分布方式不保证每层中节点之间的最小距离，但在实际应用中，由于向量数量增加，图结构质量通常较高。
>
> 5. 总结
>
> HNSW通过多层次的NSW图结构和自顶向下的搜索策略，实现了高效的近邻搜索。插入过程通过逐层查找最近邻居并创建链接，确保了图结构的动态更新。层间概率分布机制保证了图结构的整体质量，尽管在某些情况下可能出现图结构不佳的情况，但概率极低。

<img src="https://pve.digikamc.cn:8343/i/2024/11/25/ndwa1t-0.png" alt="image-20241125141405303" style="zoom:25%;" />

#### HNSW 算法的实现

```python
#Wait to study
```





